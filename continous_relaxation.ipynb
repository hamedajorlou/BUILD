{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import json\n",
    "import dag_utils as utils\n",
    "from Baselines import Nonneg_dagma, MetMulDagma\n",
    "from TopoGreedy import BUILD\n",
    "from Baselines import colide_ev\n",
    "from Baselines import DAGMA_linear\n",
    "from Baselines import notears_linear\n",
    "\n",
    "\n",
    "PATH = './results/samples/'\n",
    "SAVE = True \n",
    "SEED = 10\n",
    "N_CPUS = os.cpu_count()  # Get number of available CPUs\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_jsonable(obj):\n",
    "    \"\"\"Recursively convert objects to JSON-serializable forms.\"\"\"\n",
    "    import numpy as np\n",
    "    from pathlib import Path\n",
    "\n",
    "    # basic types\n",
    "    if obj is None or isinstance(obj, (bool, int, float, str)):\n",
    "        # cast numpy scalars to Python\n",
    "        if isinstance(obj, (np.bool_, np.integer, np.floating)):\n",
    "            return obj.item()\n",
    "        return obj\n",
    "\n",
    "    # numpy arrays -> lists (careful for huge arrays: we won't dump arrays here)\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "\n",
    "    # tuples -> lists\n",
    "    if isinstance(obj, tuple):\n",
    "        return [_to_jsonable(x) for x in obj]\n",
    "\n",
    "    # lists\n",
    "    if isinstance(obj, list):\n",
    "        return [_to_jsonable(x) for x in obj]\n",
    "\n",
    "    # dicts\n",
    "    if isinstance(obj, dict):\n",
    "        return {str(k): _to_jsonable(v) for k, v in obj.items()}\n",
    "\n",
    "    # Path\n",
    "    if isinstance(obj, Path):\n",
    "        return str(obj)\n",
    "\n",
    "    # dataclasses\n",
    "    try:\n",
    "        from dataclasses import is_dataclass, asdict\n",
    "        if is_dataclass(obj):\n",
    "            return _to_jsonable(asdict(obj))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # callables (functions/classes)\n",
    "    if callable(obj):\n",
    "        # try to capture module + name; fall back to repr\n",
    "        name = getattr(obj, \"__name__\", obj.__class__.__name__)\n",
    "        mod  = getattr(obj, \"__module__\", None)\n",
    "        return f\"{mod+'.' if mod else ''}{name}\"\n",
    "\n",
    "    # objects with __dict__\n",
    "    if hasattr(obj, \"__dict__\"):\n",
    "        return _to_jsonable(vars(obj))\n",
    "\n",
    "    # fallback\n",
    "    return repr(obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Structured DAG experiments: data simulation -> baselines -> metrics -> save & return.\n",
    "\n",
    "Assumes the following are available in scope or importable:\n",
    "  - utils.simulate_sem, utils.standarize, utils.to_bin, utils.count_accuracy, utils.compute_norm_sq_err\n",
    "  - TopoGreedy_refresh, DAGMA_linear, colide_ev, notears_linear\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, json, math, uuid, shutil, datetime as dt\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "from typing import Callable, Dict, Any, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from Baselines import GOLEM_Torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Utility: lambda schedule\n",
    "# ---------------------------------------------\n",
    "def get_lambda_value(n_nodes: int, n_samples: int, times: float = 1.0) -> float:\n",
    "    \"\"\"Common λ heuristic: sqrt(log p / n) scaled by `times`.\"\"\"\n",
    "    return math.sqrt(max(1e-12, np.log(max(2, n_nodes))) / max(2, n_samples)) * times\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Baseline spec & registry\n",
    "# ---------------------------------------------\n",
    "@dataclass\n",
    "class BaselineSpec:\n",
    "    \"\"\"\n",
    "    Unified baseline configuration.\n",
    "\n",
    "    - model: either a *callable estimator* with .fit() method (class) OR a function (e.g., notears_linear)\n",
    "    - init: kwargs passed to class constructor (ignored if model is a function)\n",
    "    - args: kwargs passed to .fit(...) or function call\n",
    "    - name: label for legends / saving\n",
    "    - standardize: if True, pass standardized X to the model; else pass raw X\n",
    "    - adapt_lambda: if True, rescale 'lamb' or 'lambda1' in args per (p, n)\n",
    "    - topo_transpose: if True, transpose W_est and binarized variant to align with your downstream expectations\n",
    "                      (e.g., TopoGreedy variants that output transposed)\n",
    "    - is_topogreedy_refresh: if True, expects dict output with keys {\"prec\",\"A_est\",\"A_est_bin\"}; handled specially\n",
    "    \"\"\"\n",
    "    model: Any\n",
    "    init: Dict[str, Any] = field(default_factory=dict)\n",
    "    args: Dict[str, Any] = field(default_factory=dict)\n",
    "    name: str = \"baseline\"\n",
    "    standardize: bool = False\n",
    "    adapt_lambda: bool = False\n",
    "    topo_transpose: bool = False\n",
    "    is_topogreedy_refresh: bool = False\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Experiment runner\n",
    "# ---------------------------------------------\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    n_graphs: int\n",
    "    n_nodes: int\n",
    "    n_samples_list: List[int]\n",
    "    edge_threshold: float\n",
    "    data_params: Dict[str, Any]\n",
    "    baselines: List[BaselineSpec]\n",
    "    out_dir: str = \"./exp_results\"\n",
    "    run_tag: Optional[str] = None\n",
    "    save_intermediate: bool = True\n",
    "    seed_offset: int = 0  # to vary graph seeds across runs\n",
    "\n",
    "\n",
    "class ExperimentRunner:\n",
    "    def __init__(self, cfg: ExperimentConfig):\n",
    "        self.cfg = cfg\n",
    "        self.run_id = cfg.run_tag or dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + uuid.uuid4().hex[:6]\n",
    "        self.out_root = Path(cfg.out_dir) / self.run_id\n",
    "        self.out_root.mkdir(parents=True, exist_ok=True)\n",
    "        self._save_manifest()\n",
    "\n",
    "    # ---------- public API ----------\n",
    "    def run(self, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Orchestrates:\n",
    "          - for each graph (with new simulation seed),\n",
    "          - for each n_samples in list,\n",
    "          - for each baseline,\n",
    "            => train / estimate => metrics => save.\n",
    "\n",
    "        Returns a dict of stacked arrays ready-to-plot.\n",
    "        \"\"\"\n",
    "        B = len(self.cfg.baselines)\n",
    "        S = len(self.cfg.n_samples_list)\n",
    "        N = self.cfg.n_nodes\n",
    "        G = self.cfg.n_graphs\n",
    "\n",
    "        # Metrics tensors: (G, S, B)\n",
    "        shd = np.zeros((G, S, B))\n",
    "        tpr = np.zeros((G, S, B))\n",
    "        fdr = np.zeros((G, S, B))\n",
    "        fscore = np.zeros((G, S, B))\n",
    "        err = np.zeros((G, S, B))\n",
    "        runtime = np.zeros((G, S, B))\n",
    "        dag_count = np.zeros((G, S, B))\n",
    "        # Frobenius diff(Theta): normalized (G, S, B)\n",
    "        theta_diff = np.zeros((G, S, B))\n",
    "\n",
    "        # Store full adjacency estimates: (G, S, B, N, N)\n",
    "        W_est_all = np.zeros((G, S, B, N, N))\n",
    "        # Optional precision matrices when available (fill zeros otherwise)\n",
    "        Theta_est_all = np.zeros((G, S, B, N, N))\n",
    "\n",
    "        for g in range(G):\n",
    "            if verbose:\n",
    "                print(f\"\\n=== Graph {g+1}/{G} ===\")\n",
    "\n",
    "            # -------------------- simulate data for this graph --------------------\n",
    "            graph_seed = self.cfg.seed_offset + g\n",
    "            data_p = dict(self.cfg.data_params)\n",
    "            data_p[\"n_nodes\"] = self.cfg.n_nodes\n",
    "\n",
    "            # we will re-simulate for each n_samples (fresh X but same W_true structure per spec)\n",
    "            W_true_cache = None\n",
    "            Theta_true_cache = None\n",
    "\n",
    "            for si, n_samples in enumerate(self.cfg.n_samples_list):\n",
    "                data_p_this = dict(data_p)\n",
    "                data_p_this[\"n_samples\"] = int(n_samples)\n",
    "\n",
    "                # simulate SEM\n",
    "                W_true, _, X, Theta_true = utils.simulate_sem(**data_p_this)\n",
    "\n",
    "                # cache structure (if consistent across n_samples) — harmless if overwritten\n",
    "                W_true_cache = W_true\n",
    "                Theta_true_cache = Theta_true\n",
    "\n",
    "                X_std = utils.standarize(X) if data_p_this.get(\"standarize\", False) else X\n",
    "                W_true_bin = utils.to_bin(W_true, self.cfg.edge_threshold)\n",
    "                norm_W_true = np.linalg.norm(W_true)\n",
    "                # emp_cov = (X_std.T @ X_std) / float(X_std.shape[0])\n",
    "                emp_cov = np.cov(X_std, rowvar = False)\n",
    "                print(f\"cond: {np.linalg.cond(Theta_true)}\")\n",
    "                if verbose:\n",
    "                    print(f\"- samples={n_samples}, edges≈{np.count_nonzero(W_true_bin)}\")\n",
    "\n",
    "                # -------------------- run all baselines --------------------\n",
    "                for bi, base in enumerate(self.cfg.baselines):\n",
    "                    X_in = X_std if base.standardize else X\n",
    "                    # args per baseline (copy so we can mutate lambdas safely)\n",
    "                    args_call = dict(base.args)\n",
    "\n",
    "                    # adaptive λ scheduling\n",
    "                    if base.adapt_lambda:\n",
    "                        if \"lamb\" in args_call:\n",
    "                            args_call[\"lamb\"] = get_lambda_value(self.cfg.n_nodes, n_samples, args_call[\"lamb\"])\n",
    "                        if \"lambda1\" in args_call:\n",
    "                            args_call[\"lambda1\"] = get_lambda_value(self.cfg.n_nodes, n_samples, args_call[\"lambda1\"])\n",
    "\n",
    "                    # ---------- execute baseline ----------\n",
    "                    t0 = perf_counter()\n",
    "                    W_est, Theta_est = self._run_one_baseline(\n",
    "                        base=base,\n",
    "                        X=X_in,\n",
    "                        X_std=X_std,\n",
    "                        emp_cov=emp_cov,\n",
    "                        edge_thr=self.cfg.edge_threshold\n",
    "                    )\n",
    "                    t1 = perf_counter()\n",
    "                    \n",
    "\n",
    "                    # NaN safeguard\n",
    "                    if np.isnan(W_est).any():\n",
    "                        W_est = np.zeros_like(W_est)\n",
    "                        W_bin = np.zeros_like(W_est)\n",
    "                    else:\n",
    "                        W_bin = utils.to_bin(W_est, self.cfg.edge_threshold)\n",
    "\n",
    "                    # Optional orientation fix for TopoGreedy-like outputs\n",
    "                    if base.topo_transpose:\n",
    "                        W_est = W_est.T\n",
    "                        W_bin = W_bin.T\n",
    "\n",
    "                    # ---------- metrics ----------\n",
    "                    shd[g, si, bi], tpr[g, si, bi], fdr[g, si, bi] = utils.count_accuracy(W_true_bin, W_bin)\n",
    "                    fscore[g, si, bi] = f1_score(W_true_bin.flatten(), W_bin.flatten())\n",
    "                    err[g, si, bi] = utils.compute_norm_sq_err(W_true, W_est, norm_W_true)\n",
    "                    runtime[g, si, bi] = (t1 - t0)\n",
    "                    dag_count[g, si, bi] = 1.0 if utils.is_dag(W_bin) else 0.0\n",
    "\n",
    "                    # Theta Frobenius difference (if Theta_est available)\n",
    "                    if Theta_est is None:\n",
    "                        theta_diff[g, si, bi] = 0.0\n",
    "                    else:\n",
    "                        # num = np.linalg.norm(Theta_est - Theta_true, \"fro\")\n",
    "                        # den = np.linalg.norm(Theta_true, \"fro\") + 1e-8\n",
    "                        Theta_norm = np.linalg.norm(Theta_true, \"fro\")\n",
    "                        theta_diff[g, si, bi] = utils.compute_norm_sq_err(Theta_true, Theta_est, Theta_norm)\n",
    "\n",
    "                    # store estimates\n",
    "                    W_est_all[g, si, bi] = W_est\n",
    "                    if Theta_est is not None:\n",
    "                        Theta_est_all[g, si, bi] = Theta_est\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"  · {base.name:<18s} | SHD {shd[g,si,bi]:.1f} | F1 {fscore[g,si,bi]:.3f} | \"\n",
    "                              f\"ΘΔ {theta_diff[g,si,bi]:.3f} | {runtime[g,si,bi]:.2f}s\")\n",
    "\n",
    "\n",
    "                    # print(f\"W_est: {W_est}\")\n",
    "                # optional per-(graph,samples) checkpoint save\n",
    "                if self.cfg.save_intermediate:\n",
    "                    data = self._save_block(\n",
    "                        g=g, si=si,\n",
    "                        W_true=W_true, Theta_true=Theta_true,\n",
    "                        W_est_all=W_est_all[g, si],\n",
    "                        Theta_est_all=Theta_est_all[g, si],\n",
    "                        shd=shd[g, si], tpr=tpr[g, si], fdr=fdr[g, si],\n",
    "                        f1=fscore[g, si], err=err[g, si], rt=runtime[g, si],\n",
    "                        dags=dag_count[g, si], theta_diff=theta_diff[g, si]\n",
    "                    )\n",
    "\n",
    "        # ------------- stack & save final -------------\n",
    "        final = dict(\n",
    "            shd=shd, tpr=tpr, fdr=fdr, f1=fscore, err=err,\n",
    "            runtime=runtime, dag_count=dag_count, theta_diff=theta_diff,\n",
    "            W_est_all=W_est_all, Theta_est_all=Theta_est_all\n",
    "        )\n",
    "        np.savez_compressed(self.out_root / \"final_results.npz\", **final)\n",
    "        if verbose:\n",
    "            print(f\"\\nSaved results to: {self.out_root}\")\n",
    "\n",
    "        return final, data\n",
    "\n",
    "    # ---------- internals ----------\n",
    "    def _run_one_baseline(\n",
    "        self,\n",
    "        base: BaselineSpec,\n",
    "        X: np.ndarray,\n",
    "        X_std: np.ndarray,\n",
    "        emp_cov: np.ndarray,\n",
    "        edge_thr: float\n",
    "    ) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Runs a single baseline and returns (W_est, Theta_est or None).\n",
    "\n",
    "        Special handling:\n",
    "          - function baselines (e.g., notears_linear): W_est = f(X, **args)\n",
    "          - TopoGreedy_refresh: expects dict with {'A_est','A_est_bin','prec'}\n",
    "          - class baselines: model = cls(**init); model.fit(X, **args); W_est = model.W_est\n",
    "            If model exposes Theta via .Theta_est (or .prec), we capture it.\n",
    "        \"\"\"\n",
    "        # function-style baseline (no .fit)\n",
    "        if callable(base.model) and not hasattr(base.model, \"fit\"):\n",
    "            # handle TopoGreedy_refresh function signature:\n",
    "            if base.is_topogreedy_refresh:\n",
    "                out = base.model(X, emp_cov, **base.args)\n",
    "                W_est = out.get(\"A_est\", None)\n",
    "                Theta_est = out.get(\"prec\", None)\n",
    "                if W_est is None:\n",
    "                    W_est = np.zeros((X.shape[1], X.shape[1]))\n",
    "                return W_est, Theta_est\n",
    "            else:\n",
    "                # generic function baseline like notears_linear(X, **args)\n",
    "                W_est = base.model(X, **base.args)\n",
    "                return W_est, None\n",
    "\n",
    "        # class-style baseline\n",
    "        model = base.model(**base.init) if base.init else base.model()\n",
    "        # Try to pass standardized X only if the baseline asked for it (already handled by caller via X argument)\n",
    "        model.fit(X, **base.args)\n",
    "\n",
    "        # Extract adjacency\n",
    "        W_est = getattr(model, \"W_est\", None)\n",
    "        if W_est is None:\n",
    "            # Some models might return directly from fit (rare); fallback to zeros\n",
    "            W_est = np.zeros((X.shape[1], X.shape[1]))\n",
    "\n",
    "        # Extract precision if present\n",
    "        Theta_est = None\n",
    "        if hasattr(model, \"Theta_est\"):\n",
    "            Theta_est = getattr(model, \"Theta_est\")\n",
    "        elif hasattr(model, \"prec\"):\n",
    "            Theta_est = getattr(model, \"prec\")\n",
    "\n",
    "        # TopoGreedy (non-refresh) variants sometimes return A transposed relative to convention;\n",
    "        # caller can request topo_transpose=True in BaselineSpec to fix orientation.\n",
    "        return W_est, Theta_est\n",
    "\n",
    "    def _save_manifest(self):\n",
    "        \"\"\"Save config manifest (JSON) for reproducibility, without non-serializable fields.\"\"\"\n",
    "        # build a sanitized copy of the config:\n",
    "        #   - convert baselines to lightweight dicts (stringify the model)\n",
    "        sanitized_cfg = {\n",
    "            \"n_graphs\": self.cfg.n_graphs,\n",
    "            \"n_nodes\": self.cfg.n_nodes,\n",
    "            \"n_samples_list\": list(self.cfg.n_samples_list),\n",
    "            \"edge_threshold\": self.cfg.edge_threshold,\n",
    "            \"data_params\": _to_jsonable(self.cfg.data_params),\n",
    "            \"baselines\": [\n",
    "                {\n",
    "                    \"name\": b.name,\n",
    "                    \"model\": _to_jsonable(b.model),           # \"module.ClassOrFunc\"\n",
    "                    \"init\": _to_jsonable(b.init),\n",
    "                    \"args\": _to_jsonable(b.args),\n",
    "                    \"standardize\": b.standardize,\n",
    "                    \"adapt_lambda\": b.adapt_lambda,\n",
    "                    \"topo_transpose\": b.topo_transpose,\n",
    "                    \"is_topogreedy_refresh\": b.is_topogreedy_refresh,\n",
    "                }\n",
    "                for b in self.cfg.baselines\n",
    "            ],\n",
    "            \"out_dir\": str(self.cfg.out_dir),\n",
    "            \"run_tag\": self.cfg.run_tag,\n",
    "            \"save_intermediate\": self.cfg.save_intermediate,\n",
    "            \"seed_offset\": self.cfg.seed_offset,\n",
    "        }\n",
    "\n",
    "        manifest = {\n",
    "            \"run_id\": self.run_id,\n",
    "            \"created_at\": dt.datetime.now().isoformat(),\n",
    "            \"config\": sanitized_cfg,\n",
    "            \"python\": {\"numpy_version\": np.__version__},\n",
    "        }\n",
    "\n",
    "        (self.out_root / \"config.json\").write_text(json.dumps(_to_jsonable(manifest), indent=2))\n",
    "\n",
    "    def _save_block(\n",
    "        self, g: int, si: int,\n",
    "        W_true: np.ndarray, Theta_true: np.ndarray,\n",
    "        W_est_all: np.ndarray,\n",
    "        Theta_est_all: np.ndarray,\n",
    "        shd: np.ndarray, tpr: np.ndarray, fdr: np.ndarray,\n",
    "        f1: np.ndarray, err: np.ndarray, rt: np.ndarray,\n",
    "        dags: np.ndarray, theta_diff: np.ndarray\n",
    "    ):\n",
    "        \"\"\"Persist per-(graph, sample-index) blob for debugging/inspection.\"\"\"\n",
    "        sub = self.out_root / f\"graph_{g:03d}\" / f\"samples_{self.cfg.n_samples_list[si]}\"\n",
    "        sub.mkdir(parents=True, exist_ok=True)\n",
    "        filename = sub / \"block.npz\"\n",
    "        np.savez_compressed(\n",
    "            filename,\n",
    "            W_true=W_true, Theta_true=Theta_true,\n",
    "            W_est_all=W_est_all, Theta_est_all=Theta_est_all,\n",
    "            shd=shd, tpr=tpr, fdr=fdr, f1=f1, err=err, runtime=rt,\n",
    "            dag_count=dags, theta_diff=theta_diff\n",
    "        )\n",
    "        \n",
    "        # Return the complete data as a dictionary containing all arrays\n",
    "        return {\n",
    "            'filename': str(filename),\n",
    "            'W_true': W_true,\n",
    "            'Theta_true': Theta_true,\n",
    "            'W_est_all': W_est_all,\n",
    "            'Theta_est_all': Theta_est_all,\n",
    "            'shd': shd,\n",
    "            'tpr': tpr,\n",
    "            'fdr': fdr,\n",
    "            'f1': f1,\n",
    "            'err': err,\n",
    "            'runtime': rt,\n",
    "            'dag_count': dags,\n",
    "            'theta_diff': theta_diff\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Plotting helpers\n",
    "# ---------------------------------------------\n",
    "def plot_summary_curves(final: Dict[str, np.ndarray],\n",
    "                        cfg: ExperimentConfig,\n",
    "                        out_dir: Path,\n",
    "                        baselines: List[BaselineSpec]):\n",
    "    \"\"\"\n",
    "    final: output of runner.run(); contains arrays (G,S,B,...)\n",
    "    Plots mean ± s.e.m. across G graphs, vs n_samples, for each baseline.\n",
    "    Also adds shaded regions for 10th-90th percentiles.\n",
    "    Saves PNGs to out_dir.\n",
    "    \"\"\"\n",
    "    n_samples = np.array(cfg.n_samples_list)\n",
    "    G = final[\"shd\"].shape[0]\n",
    "    B = final[\"shd\"].shape[2]\n",
    "\n",
    "    # --- means & standard errors over graphs (axis=0) ---\n",
    "    shd_mean = final[\"shd\"].mean(axis=0)                        # (S, B)\n",
    "    shd_sem  = final[\"shd\"].std(axis=0, ddof=1) / np.sqrt(G)    # (S, B)\n",
    "    shd_p10  = np.percentile(final[\"shd\"], 10, axis=0)          # (S, B)\n",
    "    shd_p90  = np.percentile(final[\"shd\"], 90, axis=0)          # (S, B)\n",
    "\n",
    "    err_mean = final[\"err\"].mean(axis=0)                        # (S, B)\n",
    "    err_sem  = final[\"err\"].std(axis=0, ddof=1) / np.sqrt(G)    # (S, B)\n",
    "    err_p10  = np.percentile(final[\"err\"], 10, axis=0)          # (S, B)\n",
    "    err_p90  = np.percentile(final[\"err\"], 90, axis=0)          # (S, B)\n",
    "\n",
    "    # --- SHD vs #samples ---\n",
    "    plt.figure()\n",
    "    for bi in range(B):\n",
    "        plt.errorbar(\n",
    "            n_samples, shd_mean[:, bi], yerr=shd_sem[:, bi],\n",
    "            marker='o', capsize=4, linewidth=2, label=baselines[bi].name\n",
    "        )\n",
    "        plt.fill_between(\n",
    "            n_samples, shd_p10[:, bi], shd_p90[:, bi],\n",
    "            alpha=0.2\n",
    "        )\n",
    "    plt.xlabel(\"Number of samples (n)\")\n",
    "    plt.ylabel(\"SHD (mean ± s.e.m. over graphs)\")\n",
    "    plt.title(\"SHD vs #Samples\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_dir / \"shd_vs_samples.png\", dpi=200)\n",
    "\n",
    "    # --- Normalized MSE (weights) vs #samples ---\n",
    "    plt.figure()\n",
    "    for bi in range(B):\n",
    "        plt.errorbar(\n",
    "            n_samples, err_mean[:, bi], yerr=err_sem[:, bi],\n",
    "            marker='o', capsize=4, linewidth=2, label=baselines[bi].name\n",
    "        )\n",
    "        plt.fill_between(\n",
    "            n_samples, err_p10[:, bi], err_p90[:, bi],\n",
    "            alpha=0.2\n",
    "        )\n",
    "    plt.xlabel(\"Number of samples (n)\")\n",
    "    plt.ylabel(\"Normalized MSE of weights (mean ± s.e.m.)\")\n",
    "    plt.title(\"Normalized MSE (weights) vs #Samples\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_dir / \"nmse_vs_samples.png\", dpi=200)\n",
    "\n",
    "    print(f\"Saved plots to: {out_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine the npz file\n",
    "import numpy as np\n",
    "\n",
    "# data = np.load('/Users/hamedajorlou/Documents/GROW/exp_results/20250917_004018_f9a912/graph_000/samples_1000/block.npz')\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def _sanitize_baseline_name_for_csv(name: str) -> str:\n",
    "    \"\"\"Turn 'TopoGreedy-0.02' -> 'TopoGreedy_0_02' (safe CSV column header).\"\"\"\n",
    "    return re.sub(r'[^A-Za-z0-9]+', '_', name).strip('_')\n",
    "\n",
    "def save_summary_csvs(final: Dict[str, np.ndarray],\n",
    "                      cfg: ExperimentConfig,\n",
    "                      out_dir: Path,\n",
    "                      baselines: List[BaselineSpec]) -> Tuple[Path, Path, Path]:\n",
    "    \"\"\"\n",
    "    Writes three CSVs into out_dir:\n",
    "      - shd_vs_samples.csv:   n_samples + (mean, sem, p10, p90) per baseline\n",
    "      - nmse_vs_samples.csv:  n_samples + (mean, sem, p10, p90) per baseline\n",
    "      - summary_vs_samples.csv: wide table with both metrics interleaved\n",
    "    Returns the three paths.\n",
    "    \"\"\"\n",
    "    import csv\n",
    "    n_samples = np.asarray(cfg.n_samples_list)\n",
    "    G = int(final[\"shd\"].shape[0])\n",
    "    B = int(final[\"shd\"].shape[2])\n",
    "\n",
    "    # Means, SEM, and percentiles across graphs\n",
    "    def _sem(arr):\n",
    "        return (arr.std(axis=0, ddof=1) / np.sqrt(G)) if G > 1 else np.zeros_like(arr.mean(axis=0))\n",
    "\n",
    "    shd_mean = final[\"shd\"].mean(axis=0)   # (S,B)\n",
    "    shd_sem  = _sem(final[\"shd\"])          # (S,B)\n",
    "    shd_p10  = np.percentile(final[\"shd\"], 10, axis=0)  # (S,B)\n",
    "    shd_p90  = np.percentile(final[\"shd\"], 90, axis=0)  # (S,B)\n",
    "    \n",
    "    nmse_mean = final[\"err\"].mean(axis=0)  # (S,B) normalized MSE of weights\n",
    "    nmse_sem  = _sem(final[\"err\"])         # (S,B)\n",
    "    nmse_p10  = np.percentile(final[\"err\"], 10, axis=0)  # (S,B)\n",
    "    nmse_p90  = np.percentile(final[\"err\"], 90, axis=0)  # (S,B)\n",
    "\n",
    "    # Make header labels safe for CSV/pgfplots\n",
    "    safe_names = [_sanitize_baseline_name_for_csv(b.name) for b in baselines]\n",
    "    orig_names = [b.name for b in baselines]  # for reference/legends\n",
    "\n",
    "    # 1) SHD CSV\n",
    "    shd_csv = out_dir / \"shd_vs_samples.csv\"\n",
    "    with open(shd_csv, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        header = [\"n_samples\"]\n",
    "        for sn in safe_names:\n",
    "            header += [f\"{sn}_mean\", f\"{sn}_sem\", f\"{sn}_p10\", f\"{sn}_p90\"]\n",
    "        w.writerow(header)\n",
    "        for si, n in enumerate(n_samples):\n",
    "            row = [int(n)]\n",
    "            for bi in range(B):\n",
    "                row += [float(shd_mean[si, bi]), float(shd_sem[si, bi]),\n",
    "                        float(shd_p10[si, bi]), float(shd_p90[si, bi])]\n",
    "            w.writerow(row)\n",
    "\n",
    "    # 2) NMSE CSV\n",
    "    nmse_csv = out_dir / \"nmse_vs_samples.csv\"\n",
    "    with open(nmse_csv, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        header = [\"n_samples\"]\n",
    "        for sn in safe_names:\n",
    "            header += [f\"{sn}_mean\", f\"{sn}_sem\", f\"{sn}_p10\", f\"{sn}_p90\"]\n",
    "        w.writerow(header)\n",
    "        for si, n in enumerate(n_samples):\n",
    "            row = [int(n)]\n",
    "            for bi in range(B):\n",
    "                row += [float(nmse_mean[si, bi]), float(nmse_sem[si, bi]),\n",
    "                        float(nmse_p10[si, bi]), float(nmse_p90[si, bi])]\n",
    "            w.writerow(row)\n",
    "\n",
    "    # 3) Wide summary CSV (both metrics interleaved)\n",
    "    summary_csv = out_dir / \"summary_vs_samples.csv\"\n",
    "    with open(summary_csv, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        header = [\"n_samples\"]\n",
    "        for sn in safe_names:\n",
    "            header += [f\"{sn}_SHD_mean\", f\"{sn}_SHD_sem\", f\"{sn}_SHD_p10\", f\"{sn}_SHD_p90\",\n",
    "                       f\"{sn}_NMSE_mean\", f\"{sn}_NMSE_sem\", f\"{sn}_NMSE_p10\", f\"{sn}_NMSE_p90\"]\n",
    "        w.writerow(header)\n",
    "        for si, n in enumerate(n_samples):\n",
    "            row = [int(n)]\n",
    "            for bi in range(B):\n",
    "                row += [float(shd_mean[si, bi]),  float(shd_sem[si, bi]),\n",
    "                        float(shd_p10[si, bi]),   float(shd_p90[si, bi]),\n",
    "                        float(nmse_mean[si, bi]), float(nmse_sem[si, bi]),\n",
    "                        float(nmse_p10[si, bi]),  float(nmse_p90[si, bi])]\n",
    "            w.writerow(row)\n",
    "\n",
    "    print(\"Saved CSVs:\", shd_csv, nmse_csv, summary_csv, sep=\"\\n- \")\n",
    "    return shd_csv, nmse_csv, summary_csv\n",
    "\n",
    "\n",
    "# save_summary_csvs(final, cfg, out_dir, baselines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Graph 1/10 ===\n",
      "cond: 69035.28868982918\n",
      "- samples=500, edges≈71\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "compute_norm_sq_err() missing 1 required positional argument: 'norm_W_true'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 108\u001b[0m\n\u001b[1;32m    106\u001b[0m runner \u001b[38;5;241m=\u001b[39m ExperimentRunner(cfg)\n\u001b[1;32m    107\u001b[0m t0 \u001b[38;5;241m=\u001b[39m perf_counter()\n\u001b[0;32m--> 108\u001b[0m final, data \u001b[38;5;241m=\u001b[39m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m t1 \u001b[38;5;241m=\u001b[39m perf_counter()\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m----- Completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(t1\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mt0)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes -----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 198\u001b[0m, in \u001b[0;36mExperimentRunner.run\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    194\u001b[0m     theta_diff[g, si, bi] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;66;03m# num = np.linalg.norm(Theta_est - Theta_true, \"fro\")\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# den = np.linalg.norm(Theta_true, \"fro\") + 1e-8\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     theta_diff[g, si, bi] \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_norm_sq_err\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTheta_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTheta_est\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# store estimates\u001b[39;00m\n\u001b[1;32m    201\u001b[0m W_est_all[g, si, bi] \u001b[38;5;241m=\u001b[39m W_est\n",
      "\u001b[0;31mTypeError\u001b[0m: compute_norm_sq_err() missing 1 required positional argument: 'norm_W_true'"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# Example: configure and run (10 trials; multi n)\n",
    "# ---------------------------------------------\n",
    "# from TopoGreedy import TopoGreedy_refresh_3, TopoGreedy_refresh_2\n",
    "try:\n",
    "    _to_jsonable\n",
    "except NameError:\n",
    "    def _to_jsonable(x):\n",
    "        try:\n",
    "            json.dumps(x)\n",
    "            return x\n",
    "        except Exception:\n",
    "            if hasattr(x, \"__name__\"):  # functions/classes\n",
    "                return f\"{getattr(x, '__module__', '')}.{x.__name__}\"\n",
    "            return str(x)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Data generation parameters        \n",
    "# ---------------------------------------------\n",
    "data_p = {\n",
    "    \"graph_type\": \"er\",\n",
    "    \"n_nodes\": 20,\n",
    "    \"edges\": 4 * 20,\n",
    "    \"edge_type\": \"weighted\",\n",
    "    \"w_range\": ((-2,-0.5) ,(0.5, 2.0)),\n",
    "    \"var\": 1.0,\n",
    "    # \"standarize\": True,  # if you want to standardize data before emp_cov\n",
    "}\n",
    "\n",
    "# ---------------------------------------------\n",
    "Exps = [\n",
    "    # BaselineSpec(\n",
    "    #     model=BUILD,\n",
    "    #     args={\"k_list\": [data_p[\"n_nodes\"]], \"threshold_list\": [1e-4], \"topo_thr\": 0.5, \"refresh_every\": 0.005},\n",
    "    #     name=\"BUILD-0.005\",\n",
    "    #     standardize=False,\n",
    "    #     adapt_lambda=False,\n",
    "    #     topo_transpose=False,\n",
    "    #     is_topogreedy_refresh=True\n",
    "    # ),\n",
    "    # BaselineSpec(\n",
    "    #     model=BUILD,\n",
    "    #     args={\"k_list\": [data_p[\"n_nodes\"]], \"threshold_list\": [1e-4], \"topo_thr\": 0.5, \"refresh_every\": 0.01},\n",
    "    #     name=\"BUILD-0.01\",\n",
    "    #     standardize=False,\n",
    "    #     adapt_lambda=False,\n",
    "    #     topo_transpose=False,\n",
    "    #     is_topogreedy_refresh=True\n",
    "    # ),\n",
    "    BaselineSpec(\n",
    "        model=BUILD,\n",
    "        args={\"k_list\": [data_p[\"n_nodes\"]], \"threshold_list\": [1e-4], \"topo_thr\": 0.5, \"refresh_every\": 0.1},\n",
    "        name=\"BUILD-0.02\",\n",
    "        standardize=False,\n",
    "        adapt_lambda=False,\n",
    "        topo_transpose=False,\n",
    "        is_topogreedy_refresh=True\n",
    "    ),\n",
    "    # BaselineSpec(\n",
    "    #     model=BUILD,\n",
    "    #     args={\"k_list\": [data_p[\"n_nodes\"]], \"threshold_list\": [1e-4], \"topo_thr\": 0.5, \"refresh_every\": 0.04},\n",
    "    #     name=\"BUILD-0.04\",\n",
    "    #     standardize=False,\n",
    "    #     adapt_lambda=False,\n",
    "    #     topo_transpose=False,\n",
    "    #     is_topogreedy_refresh=True\n",
    "    # ),\n",
    "    #########################################################\n",
    "    BaselineSpec(\n",
    "        model=colide_ev,\n",
    "        init={},\n",
    "        args={\"lambda1\": 0.05, \"T\": 4, \"s\": [1.0, 0.9, 0.8, 0.7],\n",
    "              \"warm_iter\": 2e4, \"max_iter\": 7e4, \"lr\": 3e-4,\n",
    "              \"disable_tqdm\": True},\n",
    "        name=\"CoLiDe-Fix\",\n",
    "        standardize=False,\n",
    "        adapt_lambda=False,\n",
    "        topo_transpose=False\n",
    "    ),\n",
    "    BaselineSpec(\n",
    "        model=DAGMA_linear,\n",
    "        init={\"loss_type\": \"l2\"},\n",
    "        args={\"lambda1\": 0.05, \"T\": 4, \"s\": [1.0, 0.9, 0.8, 0.8],\n",
    "              \"warm_iter\": 2e4, \"max_iter\": 7e4, \"lr\": 3e-4,\n",
    "              \"disable_tqdm\": True},\n",
    "        name=\"DAGMA\",\n",
    "        standardize=False,\n",
    "        adapt_lambda=False,\n",
    "        topo_transpose=False\n",
    "    ),\n",
    "]\n",
    "# IMPORTANT: multiple sample sizes + 10 trials\n",
    "cfg = ExperimentConfig(\n",
    "    n_graphs=10,                                 # mean over 10 trials\n",
    "    n_nodes=data_p[\"n_nodes\"],\n",
    "    n_samples_list=[500, 600, 700, 800, 900, 1000], # choose what you like\n",
    "    edge_threshold=0.5,\n",
    "    data_params=data_p,\n",
    "    baselines=Exps,\n",
    "    out_dir=\"./exp_results\",\n",
    "    run_tag=None,\n",
    "    save_intermediate=True,\n",
    "    seed_offset=0\n",
    ")\n",
    "\n",
    "runner = ExperimentRunner(cfg)\n",
    "t0 = perf_counter()\n",
    "final, data = runner.run(verbose=True)\n",
    "t1 = perf_counter()\n",
    "print(f\"\\n----- Completed in {(t1 - t0)/60:.3f} minutes -----\")\n",
    "\n",
    "# Create the two plots and save them next to final_results.npz\n",
    "plot_summary_curves(final, cfg, runner.out_root, cfg.baselines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Graph 1/20 ===\n",
      "cond: 70386725.24646494\n",
      "- samples=1000, edges≈770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  · BUILD-0.005        | SHD 18.0 | F1 0.988 | ΘΔ 0.307 | 1264.58s\n",
      "  · BUILD-0.01         | SHD 44.0 | F1 0.972 | ΘΔ 0.307 | 638.75s\n",
      "  · BUILD-0.02         | SHD 74.0 | F1 0.952 | ΘΔ 0.307 | 331.31s\n",
      "  · BUILD-0.04         | SHD 114.0 | F1 0.927 | ΘΔ 0.307 | 187.51s\n",
      "  · CoLiDe-Fix         | SHD 103.0 | F1 0.929 | ΘΔ 0.000 | 100.60s\n",
      "  · DAGMA              | SHD 125.0 | F1 0.914 | ΘΔ 0.000 | 88.80s\n",
      "\n",
      "=== Graph 2/20 ===\n",
      "cond: 34439038.30595772\n",
      "- samples=1000, edges≈783\n",
      "  · BUILD-0.005        | SHD 15.0 | F1 0.990 | ΘΔ 0.312 | 1269.68s\n",
      "  · BUILD-0.01         | SHD 33.0 | F1 0.979 | ΘΔ 0.312 | 664.25s\n",
      "  · BUILD-0.02         | SHD 107.0 | F1 0.933 | ΘΔ 0.312 | 341.24s\n",
      "  · BUILD-0.04         | SHD 117.0 | F1 0.927 | ΘΔ 0.312 | 181.06s\n",
      "  · CoLiDe-Fix         | SHD 99.0 | F1 0.932 | ΘΔ 0.000 | 80.99s\n",
      "  · DAGMA              | SHD 104.0 | F1 0.929 | ΘΔ 0.000 | 71.03s\n",
      "\n",
      "=== Graph 3/20 ===\n",
      "cond: 257115634.60239965\n",
      "- samples=1000, edges≈775\n",
      "  · BUILD-0.005        | SHD 12.0 | F1 0.992 | ΘΔ 0.304 | 1163.21s\n",
      "  · BUILD-0.01         | SHD 40.0 | F1 0.975 | ΘΔ 0.304 | 659.43s\n",
      "  · BUILD-0.02         | SHD 79.0 | F1 0.951 | ΘΔ 0.304 | 328.80s\n",
      "  · BUILD-0.04         | SHD 149.0 | F1 0.909 | ΘΔ 0.304 | 171.30s\n",
      "  · CoLiDe-Fix         | SHD 102.0 | F1 0.931 | ΘΔ 0.000 | 125.19s\n",
      "  · DAGMA              | SHD 132.0 | F1 0.909 | ΘΔ 0.000 | 113.37s\n",
      "\n",
      "=== Graph 4/20 ===\n",
      "cond: 1952158932.1017087\n",
      "- samples=1000, edges≈780\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 108\u001b[0m\n\u001b[1;32m    106\u001b[0m runner \u001b[38;5;241m=\u001b[39m ExperimentRunner(cfg)\n\u001b[1;32m    107\u001b[0m t0 \u001b[38;5;241m=\u001b[39m perf_counter()\n\u001b[0;32m--> 108\u001b[0m final, data \u001b[38;5;241m=\u001b[39m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m t1 \u001b[38;5;241m=\u001b[39m perf_counter()\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m----- Completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(t1\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mt0)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes -----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 163\u001b[0m, in \u001b[0;36mExperimentRunner.run\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# ---------- execute baseline ----------\u001b[39;00m\n\u001b[1;32m    162\u001b[0m t0 \u001b[38;5;241m=\u001b[39m perf_counter()\n\u001b[0;32m--> 163\u001b[0m W_est, Theta_est \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_baseline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43memp_cov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memp_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_thr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_threshold\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m t1 \u001b[38;5;241m=\u001b[39m perf_counter()\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# NaN safeguard\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 257\u001b[0m, in \u001b[0;36mExperimentRunner._run_one_baseline\u001b[0;34m(self, base, X, X_std, emp_cov, edge_thr)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(base\u001b[38;5;241m.\u001b[39mmodel) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(base\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m# handle TopoGreedy_refresh function signature:\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m base\u001b[38;5;241m.\u001b[39mis_topogreedy_refresh:\n\u001b[0;32m--> 257\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memp_cov\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m         W_est \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA_est\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    259\u001b[0m         Theta_est \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprec\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/GROW/TopoGreedy.py:418\u001b[0m, in \u001b[0;36mBUILD\u001b[0;34m(X, emp_cov, k_list, threshold_list, topo_thr, refresh_every, metric)\u001b[0m\n\u001b[1;32m    416\u001b[0m best \u001b[38;5;241m=\u001b[39m best_greedy(X, emp_cov, k_list, threshold_list, metric)\n\u001b[1;32m    417\u001b[0m Theta0 \u001b[38;5;241m=\u001b[39m best[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprec\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 418\u001b[0m A_est \u001b[38;5;241m=\u001b[39m \u001b[43mBUILDER\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTheta0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTheta0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memp_cov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memp_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbest_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mk\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_thr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mthreshold\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopo_thr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefresh_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefresh_every\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m A_est_bin \u001b[38;5;241m=\u001b[39m to_bin(A_est, thr\u001b[38;5;241m=\u001b[39mtopo_thr)\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprec\u001b[39m\u001b[38;5;124m\"\u001b[39m: Theta0, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA_est\u001b[39m\u001b[38;5;124m\"\u001b[39m: A_est, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA_est_bin\u001b[39m\u001b[38;5;124m\"\u001b[39m: A_est_bin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m: best[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthr\u001b[39m\u001b[38;5;124m\"\u001b[39m: best[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n",
      "File \u001b[0;32m~/Documents/GROW/TopoGreedy.py:104\u001b[0m, in \u001b[0;36mBUILDER\u001b[0;34m(Theta0, X, emp_cov, best_k, best_thr, edge_threshold, refresh_every, verbose)\u001b[0m\n\u001b[1;32m    102\u001b[0m         X_sub \u001b[38;5;241m=\u001b[39m X[:, active_idx]\n\u001b[1;32m    103\u001b[0m         emp_cov_sub \u001b[38;5;241m=\u001b[39m emp_cov[np\u001b[38;5;241m.\u001b[39mix_(active_idx, active_idx)]\n\u001b[0;32m--> 104\u001b[0m         Theta_refresh \u001b[38;5;241m=\u001b[39m \u001b[43mfull_greedy_and_prune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memp_cov_sub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_thr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m         Theta_aux[np\u001b[38;5;241m.\u001b[39mix_(active_idx, active_idx)] \u001b[38;5;241m=\u001b[39m Theta_refresh\n\u001b[1;32m    106\u001b[0m i \u001b[38;5;241m=\u001b[39m find_next_leaf(Theta_aux)\n",
      "File \u001b[0;32m~/Documents/GROW/Greedy_prune.py:89\u001b[0m, in \u001b[0;36mfull_greedy_and_prune\u001b[0;34m(X, emp_cov, k, threshold)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfull_greedy_and_prune\u001b[39m(X: np\u001b[38;5;241m.\u001b[39mndarray, emp_cov: np\u001b[38;5;241m.\u001b[39mndarray, k: \u001b[38;5;28mint\u001b[39m, threshold: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     88\u001b[0m     n \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 89\u001b[0m     S \u001b[38;5;241m=\u001b[39m [greedy_and_prune(X, emp_cov, i, k, threshold) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[1;32m     91\u001b[0m         S[i] \u001b[38;5;241m=\u001b[39m [j \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m S[i] \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m S[j]]\n",
      "File \u001b[0;32m~/Documents/GROW/Greedy_prune.py:89\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfull_greedy_and_prune\u001b[39m(X: np\u001b[38;5;241m.\u001b[39mndarray, emp_cov: np\u001b[38;5;241m.\u001b[39mndarray, k: \u001b[38;5;28mint\u001b[39m, threshold: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     88\u001b[0m     n \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 89\u001b[0m     S \u001b[38;5;241m=\u001b[39m [\u001b[43mgreedy_and_prune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memp_cov\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[1;32m     91\u001b[0m         S[i] \u001b[38;5;241m=\u001b[39m [j \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m S[i] \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m S[j]]\n",
      "File \u001b[0;32m~/Documents/GROW/Greedy_prune.py:85\u001b[0m, in \u001b[0;36mgreedy_and_prune\u001b[0;34m(X, emp_cov, i, k, threshold)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     S \u001b[38;5;241m=\u001b[39m greedy_old(emp_cov, k, i)\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GROW/Greedy_prune.py:72\u001b[0m, in \u001b[0;36mprune\u001b[0;34m(X, S, i, threshold)\u001b[0m\n\u001b[1;32m     70\u001b[0m v_noj \u001b[38;5;241m=\u001b[39m X_S_noj\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m x_i\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     w_noj \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM_noj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_noj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m LinAlgError:\n\u001b[1;32m     74\u001b[0m     w_noj \u001b[38;5;241m=\u001b[39m pinv(M_noj) \u001b[38;5;241m@\u001b[39m v_noj\n",
      "File \u001b[0;32m~/miniforge3/envs/Greedytopo/lib/python3.10/site-packages/numpy/linalg/_linalg.py:410\u001b[0m, in \u001b[0;36msolve\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    407\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdd->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m errstate(call\u001b[38;5;241m=\u001b[39m_raise_linalgerror_singular, invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    409\u001b[0m               over\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, divide\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, under\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 410\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(r\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# Example: configure and run (10 trials; multi n)\n",
    "# ---------------------------------------------\n",
    "# from TopoGreedy import TopoGreedy_refresh_3, TopoGreedy_refresh_2\n",
    "try:\n",
    "    _to_jsonable\n",
    "except NameError:\n",
    "    def _to_jsonable(x):\n",
    "        try:\n",
    "            json.dumps(x)\n",
    "            return x\n",
    "        except Exception:\n",
    "            if hasattr(x, \"__name__\"):  # functions/classes\n",
    "                return f\"{getattr(x, '__module__', '')}.{x.__name__}\"\n",
    "            return str(x)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Data generation parameters        \n",
    "# ---------------------------------------------\n",
    "data_p = {\n",
    "    \"graph_type\": \"er\",\n",
    "    \"n_nodes\": 200,\n",
    "    \"edges\": 4 * 200,\n",
    "    \"edge_type\": \"weighted\",\n",
    "    \"w_range\": ((-2,-0.5) ,(0.5, 2.0)),\n",
    "    \"var\": 1.0,\n",
    "    # \"standarize\": True,  # if you want to standardize data before emp_cov\n",
    "}\n",
    "\n",
    "# ---------------------------------------------\n",
    "Exps = [\n",
    "    BaselineSpec(\n",
    "        model=BUILD,\n",
    "        args={\"k_list\": [data_p[\"n_nodes\"]], \"threshold_list\": [1e-4], \"topo_thr\": 0.5, \"refresh_every\": 0.005},\n",
    "        name=\"BUILD-0.005\",\n",
    "        standardize=False,\n",
    "        adapt_lambda=False,\n",
    "        topo_transpose=False,\n",
    "        is_topogreedy_refresh=True\n",
    "    ),\n",
    "    BaselineSpec(\n",
    "        model=BUILD,\n",
    "        args={\"k_list\": [data_p[\"n_nodes\"]], \"threshold_list\": [1e-4], \"topo_thr\": 0.5, \"refresh_every\": 0.01},\n",
    "        name=\"BUILD-0.01\",\n",
    "        standardize=False,\n",
    "        adapt_lambda=False,\n",
    "        topo_transpose=False,\n",
    "        is_topogreedy_refresh=True\n",
    "    ),\n",
    "    BaselineSpec(\n",
    "        model=BUILD,\n",
    "        args={\"k_list\": [data_p[\"n_nodes\"]], \"threshold_list\": [1e-4], \"topo_thr\": 0.5, \"refresh_every\": 0.02},\n",
    "        name=\"BUILD-0.02\",\n",
    "        standardize=False,\n",
    "        adapt_lambda=False,\n",
    "        topo_transpose=False,\n",
    "        is_topogreedy_refresh=True\n",
    "    ),\n",
    "    BaselineSpec(\n",
    "        model=BUILD,\n",
    "        args={\"k_list\": [data_p[\"n_nodes\"]], \"threshold_list\": [1e-4], \"topo_thr\": 0.5, \"refresh_every\": 0.04},\n",
    "        name=\"BUILD-0.04\",\n",
    "        standardize=False,\n",
    "        adapt_lambda=False,\n",
    "        topo_transpose=False,\n",
    "        is_topogreedy_refresh=True\n",
    "    ),\n",
    "    #########################################################\n",
    "    BaselineSpec(\n",
    "        model=colide_ev,\n",
    "        init={},\n",
    "        args={\"lambda1\": 0.05, \"T\": 4, \"s\": [1.0, 0.9, 0.8, 0.7],\n",
    "              \"warm_iter\": 2e4, \"max_iter\": 7e4, \"lr\": 3e-4,\n",
    "              \"disable_tqdm\": True},\n",
    "        name=\"CoLiDe-Fix\",\n",
    "        standardize=False,\n",
    "        adapt_lambda=False,\n",
    "        topo_transpose=False\n",
    "    ),\n",
    "    BaselineSpec(\n",
    "        model=DAGMA_linear,\n",
    "        init={\"loss_type\": \"l2\"},\n",
    "        args={\"lambda1\": 0.05, \"T\": 4, \"s\": [1.0, 0.9, 0.8, 0.8],\n",
    "              \"warm_iter\": 2e4, \"max_iter\": 7e4, \"lr\": 3e-4,\n",
    "              \"disable_tqdm\": True},\n",
    "        name=\"DAGMA\",\n",
    "        standardize=False,\n",
    "        adapt_lambda=False,\n",
    "        topo_transpose=False\n",
    "    ),\n",
    "]\n",
    "# IMPORTANT: multiple sample sizes + 10 trials\n",
    "cfg = ExperimentConfig(\n",
    "    n_graphs=20,                                 # mean over 10 trials\n",
    "    n_nodes=data_p[\"n_nodes\"],\n",
    "    n_samples_list=[1000], # choose what you like\n",
    "    edge_threshold=0.5,\n",
    "    data_params=data_p,\n",
    "    baselines=Exps,\n",
    "    out_dir=\"./exp_results\",\n",
    "    run_tag=None,\n",
    "    save_intermediate=True,\n",
    "    seed_offset=0\n",
    ")\n",
    "\n",
    "runner = ExperimentRunner(cfg)\n",
    "t0 = perf_counter()\n",
    "final, data = runner.run(verbose=True)\n",
    "t1 = perf_counter()\n",
    "print(f\"\\n----- Completed in {(t1 - t0)/60:.3f} minutes -----\")\n",
    "\n",
    "# Create the two plots and save them next to final_results.npz\n",
    "plot_summary_curves(final, cfg, runner.out_root, cfg.baselines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine the npz file\n",
    "import numpy as np\n",
    "\n",
    "# data = np.load('/Users/hamedajorlou/Documents/GROW/exp_results/20250917_004018_f9a912/graph_000/samples_1000/block.npz')\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def _sanitize_baseline_name_for_csv(name: str) -> str:\n",
    "    \"\"\"Turn 'TopoGreedy-0.02' -> 'TopoGreedy_0_02' (safe CSV column header).\"\"\"\n",
    "    return re.sub(r'[^A-Za-z0-9]+', '_', name).strip('_')\n",
    "\n",
    "def save_summary_csvs(final: Dict[str, np.ndarray],\n",
    "                      cfg: ExperimentConfig,\n",
    "                      out_dir: Path,\n",
    "                      baselines: List[BaselineSpec]) -> Tuple[Path, Path, Path]:\n",
    "    \"\"\"\n",
    "    Writes three CSVs into out_dir:\n",
    "      - shd_vs_samples.csv:   n_samples + (mean, sem, p10, p90) per baseline\n",
    "      - nmse_vs_samples.csv:  n_samples + (mean, sem, p10, p90) per baseline\n",
    "      - summary_vs_samples.csv: wide table with both metrics interleaved\n",
    "    Returns the three paths.\n",
    "    \"\"\"\n",
    "    import csv\n",
    "    n_samples = np.asarray(cfg.n_samples_list)\n",
    "    G = int(final[\"shd\"].shape[0])\n",
    "    B = int(final[\"shd\"].shape[2])\n",
    "\n",
    "    # Means, SEM, and percentiles across graphs\n",
    "    def _sem(arr):\n",
    "        return (arr.std(axis=0, ddof=1) / np.sqrt(G)) if G > 1 else np.zeros_like(arr.mean(axis=0))\n",
    "\n",
    "    shd_mean = final[\"shd\"].mean(axis=0)   # (S,B)\n",
    "    shd_sem  = _sem(final[\"shd\"])          # (S,B)\n",
    "    shd_p10  = np.percentile(final[\"shd\"], 10, axis=0)  # (S,B)\n",
    "    shd_p90  = np.percentile(final[\"shd\"], 90, axis=0)  # (S,B)\n",
    "    \n",
    "    nmse_mean = final[\"err\"].mean(axis=0)  # (S,B) normalized MSE of weights\n",
    "    nmse_sem  = _sem(final[\"err\"])         # (S,B)\n",
    "    nmse_p10  = np.percentile(final[\"err\"], 10, axis=0)  # (S,B)\n",
    "    nmse_p90  = np.percentile(final[\"err\"], 90, axis=0)  # (S,B)\n",
    "\n",
    "    # Make header labels safe for CSV/pgfplots\n",
    "    safe_names = [_sanitize_baseline_name_for_csv(b.name) for b in baselines]\n",
    "    orig_names = [b.name for b in baselines]  # for reference/legends\n",
    "\n",
    "    # 1) SHD CSV\n",
    "    shd_csv = out_dir / \"shd_vs_samples.csv\"\n",
    "    with open(shd_csv, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        header = [\"n_samples\"]\n",
    "        for sn in safe_names:\n",
    "            header += [f\"{sn}_mean\", f\"{sn}_sem\", f\"{sn}_p10\", f\"{sn}_p90\"]\n",
    "        w.writerow(header)\n",
    "        for si, n in enumerate(n_samples):\n",
    "            row = [int(n)]\n",
    "            for bi in range(B):\n",
    "                row += [float(shd_mean[si, bi]), float(shd_sem[si, bi]),\n",
    "                        float(shd_p10[si, bi]), float(shd_p90[si, bi])]\n",
    "            w.writerow(row)\n",
    "\n",
    "    # 2) NMSE CSV\n",
    "    nmse_csv = out_dir / \"nmse_vs_samples.csv\"\n",
    "    with open(nmse_csv, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        header = [\"n_samples\"]\n",
    "        for sn in safe_names:\n",
    "            header += [f\"{sn}_mean\", f\"{sn}_sem\", f\"{sn}_p10\", f\"{sn}_p90\"]\n",
    "        w.writerow(header)\n",
    "        for si, n in enumerate(n_samples):\n",
    "            row = [int(n)]\n",
    "            for bi in range(B):\n",
    "                row += [float(nmse_mean[si, bi]), float(nmse_sem[si, bi]),\n",
    "                        float(nmse_p10[si, bi]), float(nmse_p90[si, bi])]\n",
    "            w.writerow(row)\n",
    "\n",
    "    # 3) Wide summary CSV (both metrics interleaved)\n",
    "    summary_csv = out_dir / \"summary_vs_samples.csv\"\n",
    "    with open(summary_csv, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        header = [\"n_samples\"]\n",
    "        for sn in safe_names:\n",
    "            header += [f\"{sn}_SHD_mean\", f\"{sn}_SHD_sem\", f\"{sn}_SHD_p10\", f\"{sn}_SHD_p90\",\n",
    "                       f\"{sn}_NMSE_mean\", f\"{sn}_NMSE_sem\", f\"{sn}_NMSE_p10\", f\"{sn}_NMSE_p90\"]\n",
    "        w.writerow(header)\n",
    "        for si, n in enumerate(n_samples):\n",
    "            row = [int(n)]\n",
    "            for bi in range(B):\n",
    "                row += [float(shd_mean[si, bi]),  float(shd_sem[si, bi]),\n",
    "                        float(shd_p10[si, bi]),   float(shd_p90[si, bi]),\n",
    "                        float(nmse_mean[si, bi]), float(nmse_sem[si, bi]),\n",
    "                        float(nmse_p10[si, bi]),  float(nmse_p90[si, bi])]\n",
    "            w.writerow(row)\n",
    "\n",
    "    print(\"Saved CSVs:\", shd_csv, nmse_csv, summary_csv, sep=\"\\n- \")\n",
    "    return shd_csv, nmse_csv, summary_csv\n",
    "\n",
    "\n",
    "# save_summary_csvs(final, cfg, out_dir, baselines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfmt\u001b[39m(m, s):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mean, std \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(m, s)]\n\u001b[0;32m---> 28\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBaseline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSHD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshd_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshd_std\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFDR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfdr_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfdr_std\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTPR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtpr_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtpr_std\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mF1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf1_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf1_std\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTime (s)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruntime_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mruntime_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Display nicely as a table\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display, HTML\n",
      "File \u001b[0;32m~/miniforge3/envs/Greedytopo/lib/python3.10/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/miniforge3/envs/Greedytopo/lib/python3.10/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/Greedytopo/lib/python3.10/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/miniforge3/envs/Greedytopo/lib/python3.10/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# Convert the npz results to CSV format suitable for LaTeX/Overleaf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "results_path = \"exp_results/20250913_012501_25daf9/final_results.npz\"\n",
    "final = dict(np.load(results_path, allow_pickle=True))\n",
    "\n",
    "# Assume Exps is available and contains the baseline names in order\n",
    "baseline_names = [b.name for b in Exps]\n",
    "\n",
    "# Compute means and stds over axis 0 (n_graphs)\n",
    "shd_mean = final[\"shd\"].mean(axis=0).squeeze()\n",
    "shd_std = final[\"shd\"].std(axis=0).squeeze()\n",
    "fdr_mean = final[\"fdr\"].mean(axis=0).squeeze()\n",
    "fdr_std = final[\"fdr\"].std(axis=0).squeeze()\n",
    "tpr_mean = final[\"tpr\"].mean(axis=0).squeeze()\n",
    "tpr_std = final[\"tpr\"].std(axis=0).squeeze()\n",
    "f1_mean = final[\"f1\"].mean(axis=0).squeeze()\n",
    "f1_std = final[\"f1\"].std(axis=0).squeeze()\n",
    "runtime_mean = final[\"runtime\"].mean(axis=0).squeeze()\n",
    "runtime_std = final[\"runtime\"].std(axis=0).squeeze()\n",
    "\n",
    "# Format as \"mean ± std\" with 2 decimals\n",
    "def fmt(m, s):\n",
    "    return [f\"{mean:.2f} ± {std:.2f}\" for mean, std in zip(m, s)]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Baseline\": baseline_names,\n",
    "    \"SHD\": fmt(shd_mean, shd_std),\n",
    "    \"FDR\": fmt(fdr_mean, fdr_std),\n",
    "    \"TPR\": fmt(tpr_mean, tpr_std),\n",
    "    \"F1\": fmt(f1_mean, f1_std),\n",
    "    \"Time (s)\": fmt(runtime_mean, runtime_std)\n",
    "})\n",
    "\n",
    "# Display nicely as a table\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(df.to_html(index=False, escape=False)))\n",
    "\n",
    "\n",
    "# Create a LaTeX-friendly summary table\n",
    "summary_data = []\n",
    "for b, baseline in enumerate(baseline_names):\n",
    "    # Format numbers for LaTeX with proper precision\n",
    "    summary_row = {\n",
    "        'Baseline': baseline.replace('_', '\\\\_'),  # Escape underscores for LaTeX\n",
    "        'SHD': f\"{shd_mean[b]:.2f} $\\\\pm$ {shd_std[b]:.2f}\",\n",
    "        'FDR': f\"{fdr_mean[b]:.3f} $\\\\pm$ {fdr_std[b]:.3f}\",\n",
    "        'TPR': f\"{tpr_mean[b]:.3f} $\\\\pm$ {tpr_std[b]:.3f}\",\n",
    "        'F1': f\"{f1_mean[b]:.3f} $\\\\pm$ {f1_std[b]:.3f}\",\n",
    "        'Time (s)': f\"{runtime_mean[b]:.2f} $\\\\pm$ {runtime_std[b]:.2f}\"\n",
    "    }\n",
    "    summary_data.append(summary_row)\n",
    "\n",
    "# Create DataFrame for LaTeX table\n",
    "latex_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Save LaTeX-friendly CSV\n",
    "latex_csv_path = results_path.replace('.npz', '_latex.csv')\n",
    "latex_df.to_csv(latex_csv_path, index=False)\n",
    "print(f\"LaTeX-friendly results saved to: {latex_csv_path}\")\n",
    "\n",
    "# Also create a raw data CSV with separate mean/std columns for more flexibility\n",
    "raw_summary_data = []\n",
    "for b, baseline in enumerate(baseline_names):\n",
    "    raw_row = {\n",
    "        'baseline': baseline,\n",
    "        'shd_mean': f\"{shd_mean[b]:.2f}\",\n",
    "        'shd_std': f\"{shd_std[b]:.2f}\",\n",
    "        'fdr_mean': f\"{fdr_mean[b]:.3f}\",\n",
    "        'fdr_std': f\"{fdr_std[b]:.3f}\",\n",
    "        'tpr_mean': f\"{tpr_mean[b]:.3f}\",\n",
    "        'tpr_std': f\"{tpr_std[b]:.3f}\",\n",
    "        'f1_mean': f\"{f1_mean[b]:.3f}\",\n",
    "        'f1_std': f\"{f1_std[b]:.3f}\",\n",
    "        'runtime_mean': f\"{runtime_mean[b]:.2f}\",\n",
    "        'runtime_std': f\"{runtime_std[b]:.2f}\"\n",
    "    }\n",
    "    raw_summary_data.append(raw_row)\n",
    "\n",
    "raw_df = pd.DataFrame(raw_summary_data)\n",
    "raw_csv_path = results_path.replace('.npz', '_raw_summary.csv')\n",
    "raw_df.to_csv(raw_csv_path, index=False)\n",
    "print(f\"Raw summary results saved to: {raw_csv_path}\")\n",
    "\n",
    "# Display the LaTeX-ready table\n",
    "print(\"\\nLaTeX-ready table:\")\n",
    "print(latex_df.to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
