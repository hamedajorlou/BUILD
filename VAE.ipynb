{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Simple VAE architecture\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=2):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 200),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(200, latent_dim)\n",
    "        self.fc_var = nn.Linear(200, latent_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_var(h)\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "\n",
    "# Fun Fact 1: Let's visualize how VAE learns a continuous latent space\n",
    "def plot_latent_space(vae, loader, num_batches=100):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    with torch.no_grad():\n",
    "        for i, (data, labels) in enumerate(loader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "            mu, _ = vae.encode(data.view(-1, 784))\n",
    "            plt.scatter(mu[:, 0], mu[:, 1], c=labels, cmap='tab10', alpha=0.6)\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Latent Space Distribution\")\n",
    "    plt.show()\n",
    "\n",
    "# Fun Fact 2: Let's see how VAE interpolates between digits\n",
    "def interpolate_digits(vae, loader, start_digit=1, end_digit=7):\n",
    "    with torch.no_grad():\n",
    "        # Get examples of start and end digits\n",
    "        start_img = None\n",
    "        end_img = None\n",
    "        for data, labels in loader:\n",
    "            if start_img is None and start_digit in labels:\n",
    "                start_img = data[labels == start_digit][0]\n",
    "            if end_img is None and end_digit in labels:\n",
    "                end_img = data[labels == end_digit][0]\n",
    "            if start_img is not None and end_img is not None:\n",
    "                break\n",
    "        \n",
    "        # Get latent representations\n",
    "        start_mu, _ = vae.encode(start_img.view(-1, 784))\n",
    "        end_mu, _ = vae.encode(end_img.view(-1, 784))\n",
    "        \n",
    "        # Create interpolations\n",
    "        steps = 10\n",
    "        interpolations = []\n",
    "        for alpha in torch.linspace(0, 1, steps):\n",
    "            z = start_mu * (1-alpha) + end_mu * alpha\n",
    "            interpolated = vae.decoder(z)\n",
    "            interpolations.append(interpolated.view(28, 28))\n",
    "            \n",
    "        # Plot\n",
    "        plt.figure(figsize=(15, 3))\n",
    "        for i in range(steps):\n",
    "            plt.subplot(1, steps, i+1)\n",
    "            plt.imshow(interpolations[i].cpu(), cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.suptitle(f\"Interpolation between {start_digit} and {end_digit}\")\n",
    "        plt.show()\n",
    "\n",
    "# Train VAE\n",
    "vae = VAE()\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "print(\"Training VAE for 5 epochs to demonstrate concepts...\")\n",
    "for epoch in range(5):\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, log_var = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1} completed')\n",
    "\n",
    "print(\"\\nFun Fact #1: VAE learns a continuous 2D latent space where similar digits cluster together\")\n",
    "plot_latent_space(vae, train_loader)\n",
    "\n",
    "print(\"\\nFun Fact #2: VAE can smoothly interpolate between different digits\")\n",
    "interpolate_digits(vae, train_loader, start_digit=1, end_digit=7)\n",
    "\n",
    "print(\"\\nFun Fact #3: The loss function has two parts:\")\n",
    "print(\"1. Reconstruction loss (BCE) - Makes the autoencoder reconstruct inputs well\")\n",
    "print(\"2. KL Divergence loss - Forces the latent space to approximate a normal distribution\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
