{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import json\n",
    "import dag_utils as utils\n",
    "from Baselines import Nonneg_dagma, MetMulDagma\n",
    "from Baselines import colide_ev\n",
    "from Baselines import DAGMA_linear\n",
    "from Baselines import notears_linear\n",
    "from BUILD import BUILD\n",
    "from utils import *\n",
    "PATH = './results/samples/'\n",
    "SAVE = True \n",
    "SEED = 10\n",
    "N_CPUS = os.cpu_count()  # Get number of available CPUs\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Structured DAG experiments: data simulation -> baselines -> metrics -> save & return.\n",
    "\n",
    "Assumes the following are available in scope or importable:\n",
    "  - utils.simulate_sem, utils.standarize, utils.to_bin, utils.count_accuracy, utils.compute_norm_sq_err\n",
    "  - TopoGreedy_refresh, DAGMA_linear, colide_ev, notears_linear\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, json, math, uuid, shutil, datetime as dt\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "from typing import Callable, Dict, Any, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from Baselines import GOLEM_Torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Utility: lambda schedule\n",
    "# ---------------------------------------------\n",
    "def get_lambda_value(n_nodes: int, n_samples: int, times: float = 1.0) -> float:\n",
    "    \"\"\"Common λ heuristic: sqrt(log p / n) scaled by `times`.\"\"\"\n",
    "    return math.sqrt(max(1e-12, np.log(max(2, n_nodes))) / max(2, n_samples)) * times\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Baseline spec & registry\n",
    "# ---------------------------------------------\n",
    "@dataclass\n",
    "class BaselineSpec:\n",
    "    \"\"\"\n",
    "    Unified baseline configuration.\n",
    "\n",
    "    - model: either a *callable estimator* with .fit() method (class) OR a function (e.g., notears_linear)\n",
    "    - init: kwargs passed to class constructor (ignored if model is a function)\n",
    "    - args: kwargs passed to .fit(...) or function call\n",
    "    - name: label for legends / saving\n",
    "    - standardize: if True, pass standardized X to the model; else pass raw X\n",
    "    - adapt_lambda: if True, rescale 'lamb' or 'lambda1' in args per (p, n)\n",
    "    - topo_transpose: if True, transpose W_est and binarized variant to align with your downstream expectations\n",
    "                      (e.g., TopoGreedy variants that output transposed)\n",
    "    - is_topogreedy_refresh: if True, expects dict output with keys {\"prec\",\"A_est\",\"A_est_bin\"}; handled specially\n",
    "    \"\"\"\n",
    "    model: Any\n",
    "    init: Dict[str, Any] = field(default_factory=dict)\n",
    "    args: Dict[str, Any] = field(default_factory=dict)\n",
    "    name: str = \"baseline\"\n",
    "    standardize: bool = False\n",
    "    adapt_lambda: bool = False\n",
    "    topo_transpose: bool = False\n",
    "    is_topogreedy_refresh: bool = False\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Experiment runner\n",
    "# ---------------------------------------------\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    n_graphs: int\n",
    "    n_nodes: int\n",
    "    n_samples_list: List[int]\n",
    "    edge_threshold: float\n",
    "    data_params: Dict[str, Any]\n",
    "    baselines: List[BaselineSpec]\n",
    "    out_dir: str = \"./exp_results\"\n",
    "    run_tag: Optional[str] = None\n",
    "    save_intermediate: bool = True\n",
    "    seed_offset: int = 0  # to vary graph seeds across runs\n",
    "\n",
    "\n",
    "class ExperimentRunner:\n",
    "    def __init__(self, cfg: ExperimentConfig):\n",
    "        self.cfg = cfg\n",
    "        self.run_id = cfg.run_tag or dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + uuid.uuid4().hex[:6]\n",
    "        self.out_root = Path(cfg.out_dir) / self.run_id\n",
    "        self.out_root.mkdir(parents=True, exist_ok=True)\n",
    "        self._save_manifest()\n",
    "\n",
    "    # ---------- public API ----------\n",
    "    def run(self, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Orchestrates:\n",
    "          - for each graph (with new simulation seed),\n",
    "          - for each n_samples in list,\n",
    "          - for each baseline,\n",
    "            => train / estimate => metrics => save.\n",
    "\n",
    "        Returns a dict of stacked arrays ready-to-plot.\n",
    "        \"\"\"\n",
    "        B = len(self.cfg.baselines)\n",
    "        S = len(self.cfg.n_samples_list)\n",
    "        N = self.cfg.n_nodes\n",
    "        G = self.cfg.n_graphs\n",
    "\n",
    "        # Metrics tensors: (G, S, B)\n",
    "        shd = np.zeros((G, S, B))\n",
    "        tpr = np.zeros((G, S, B))\n",
    "        fdr = np.zeros((G, S, B))\n",
    "        fscore = np.zeros((G, S, B))\n",
    "        err = np.zeros((G, S, B))\n",
    "        runtime = np.zeros((G, S, B))\n",
    "        dag_count = np.zeros((G, S, B))\n",
    "        # Frobenius diff(Theta): normalized (G, S, B)\n",
    "        theta_diff = np.zeros((G, S, B))\n",
    "\n",
    "        # Store full adjacency estimates: (G, S, B, N, N)\n",
    "        W_est_all = np.zeros((G, S, B, N, N))\n",
    "        # Optional precision matrices when available (fill zeros otherwise)\n",
    "        Theta_est_all = np.zeros((G, S, B, N, N))\n",
    "\n",
    "        for g in range(G):\n",
    "            if verbose:\n",
    "                print(f\"\\n=== Graph {g+1}/{G} ===\")\n",
    "\n",
    "            # -------------------- simulate data for this graph --------------------\n",
    "            graph_seed = self.cfg.seed_offset + g\n",
    "            data_p = dict(self.cfg.data_params)\n",
    "            data_p[\"n_nodes\"] = self.cfg.n_nodes\n",
    "\n",
    "            # we will re-simulate for each n_samples (fresh X but same W_true structure per spec)\n",
    "            W_true_cache = None\n",
    "            Theta_true_cache = None\n",
    "\n",
    "            for si, n_samples in enumerate(self.cfg.n_samples_list):\n",
    "                data_p_this = dict(data_p)\n",
    "                data_p_this[\"n_samples\"] = int(n_samples)\n",
    "\n",
    "                # simulate SEM\n",
    "                W_true, _, X, Theta_true = utils.simulate_sem(**data_p_this)\n",
    "\n",
    "                # cache structure (if consistent across n_samples) — harmless if overwritten\n",
    "                W_true_cache = W_true\n",
    "                Theta_true_cache = Theta_true\n",
    "\n",
    "                X_std = utils.standarize(X) if data_p_this.get(\"standarize\", False) else X\n",
    "                W_true_bin = utils.to_bin(W_true, self.cfg.edge_threshold)\n",
    "                norm_W_true = np.linalg.norm(W_true)\n",
    "                # emp_cov = (X_std.T @ X_std) / float(X_std.shape[0])\n",
    "                emp_cov = np.cov(X_std, rowvar = False)\n",
    "                print(f\"cond: {np.linalg.cond(Theta_true)}\")\n",
    "                if verbose:\n",
    "                    print(f\"- samples={n_samples}, edges≈{np.count_nonzero(W_true_bin)}\")\n",
    "\n",
    "                # -------------------- run all baselines --------------------\n",
    "                for bi, base in enumerate(self.cfg.baselines):\n",
    "                    X_in = X_std if base.standardize else X\n",
    "                    # args per baseline (copy so we can mutate lambdas safely)\n",
    "                    args_call = dict(base.args)\n",
    "\n",
    "                    # adaptive λ scheduling\n",
    "                    if base.adapt_lambda:\n",
    "                        if \"lamb\" in args_call:\n",
    "                            args_call[\"lamb\"] = get_lambda_value(self.cfg.n_nodes, n_samples, args_call[\"lamb\"])\n",
    "                        if \"lambda1\" in args_call:\n",
    "                            args_call[\"lambda1\"] = get_lambda_value(self.cfg.n_nodes, n_samples, args_call[\"lambda1\"])\n",
    "\n",
    "                    # ---------- execute baseline ----------\n",
    "                    t0 = perf_counter()\n",
    "                    W_est, Theta_est = self._run_one_baseline(\n",
    "                        base=base,\n",
    "                        X=X_in,\n",
    "                        X_std=X_std,\n",
    "                        emp_cov=emp_cov,\n",
    "                        edge_thr=self.cfg.edge_threshold\n",
    "                    )\n",
    "                    t1 = perf_counter()\n",
    "                    \n",
    "\n",
    "                    # NaN safeguard\n",
    "                    if np.isnan(W_est).any():\n",
    "                        W_est = np.zeros_like(W_est)\n",
    "                        W_bin = np.zeros_like(W_est)\n",
    "                    else:\n",
    "                        W_bin = utils.to_bin(W_est, self.cfg.edge_threshold)\n",
    "\n",
    "                    # Optional orientation fix for TopoGreedy-like outputs\n",
    "                    if base.topo_transpose:\n",
    "                        W_est = W_est.T\n",
    "                        W_bin = W_bin.T\n",
    "\n",
    "                    # ---------- metrics ----------\n",
    "                    shd[g, si, bi], tpr[g, si, bi], fdr[g, si, bi] = utils.count_accuracy(W_true_bin, W_bin)\n",
    "                    fscore[g, si, bi] = f1_score(W_true_bin.flatten(), W_bin.flatten())\n",
    "                    err[g, si, bi] = utils.compute_norm_sq_err(W_true, W_est, norm_W_true)\n",
    "                    runtime[g, si, bi] = (t1 - t0)\n",
    "                    dag_count[g, si, bi] = 1.0 if utils.is_dag(W_bin) else 0.0\n",
    "\n",
    "                    # Theta Frobenius difference (if Theta_est available)\n",
    "                    if Theta_est is None:\n",
    "                        theta_diff[g, si, bi] = 0.0\n",
    "                    else:\n",
    "                        # num = np.linalg.norm(Theta_est - Theta_true, \"fro\")\n",
    "                        # den = np.linalg.norm(Theta_true, \"fro\") + 1e-8\n",
    "                        Theta_norm = np.linalg.norm(Theta_true, \"fro\")\n",
    "                        theta_diff[g, si, bi] = utils.compute_norm_sq_err(Theta_true, Theta_est, Theta_norm)\n",
    "\n",
    "                    # store estimates\n",
    "                    W_est_all[g, si, bi] = W_est\n",
    "                    if Theta_est is not None:\n",
    "                        Theta_est_all[g, si, bi] = Theta_est\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"  · {base.name:<18s} | SHD {shd[g,si,bi]:.1f} | F1 {fscore[g,si,bi]:.3f} | \"\n",
    "                              f\"ΘΔ {theta_diff[g,si,bi]:.3f} | {runtime[g,si,bi]:.2f}s\")\n",
    "\n",
    "\n",
    "                    # print(f\"W_est: {W_est}\")\n",
    "                # optional per-(graph,samples) checkpoint save\n",
    "                if self.cfg.save_intermediate:\n",
    "                    data = self._save_block(\n",
    "                        g=g, si=si,\n",
    "                        W_true=W_true, Theta_true=Theta_true,\n",
    "                        W_est_all=W_est_all[g, si],\n",
    "                        Theta_est_all=Theta_est_all[g, si],\n",
    "                        shd=shd[g, si], tpr=tpr[g, si], fdr=fdr[g, si],\n",
    "                        f1=fscore[g, si], err=err[g, si], rt=runtime[g, si],\n",
    "                        dags=dag_count[g, si], theta_diff=theta_diff[g, si]\n",
    "                    )\n",
    "\n",
    "        # ------------- stack & save final -------------\n",
    "        final = dict(\n",
    "            shd=shd, tpr=tpr, fdr=fdr, f1=fscore, err=err,\n",
    "            runtime=runtime, dag_count=dag_count, theta_diff=theta_diff,\n",
    "            W_est_all=W_est_all, Theta_est_all=Theta_est_all\n",
    "        )\n",
    "        np.savez_compressed(self.out_root / \"final_results.npz\", **final)\n",
    "        if verbose:\n",
    "            print(f\"\\nSaved results to: {self.out_root}\")\n",
    "\n",
    "        return final, data\n",
    "\n",
    "    # ---------- internals ----------\n",
    "    def _run_one_baseline(\n",
    "        self,\n",
    "        base: BaselineSpec,\n",
    "        X: np.ndarray,\n",
    "        X_std: np.ndarray,\n",
    "        emp_cov: np.ndarray,\n",
    "        edge_thr: float\n",
    "    ) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Runs a single baseline and returns (W_est, Theta_est or None).\n",
    "\n",
    "        Special handling:\n",
    "          - function baselines (e.g., notears_linear): W_est = f(X, **args)\n",
    "          - TopoGreedy_refresh: expects dict with {'A_est','A_est_bin','prec'}\n",
    "          - class baselines: model = cls(**init); model.fit(X, **args); W_est = model.W_est\n",
    "            If model exposes Theta via .Theta_est (or .prec), we capture it.\n",
    "        \"\"\"\n",
    "        # function-style baseline (no .fit)\n",
    "        if callable(base.model) and not hasattr(base.model, \"fit\"):\n",
    "            # handle TopoGreedy_refresh function signature:\n",
    "            if base.is_topogreedy_refresh:\n",
    "                out = base.model(X, emp_cov, **base.args)\n",
    "                W_est = out.get(\"A_est\", None)\n",
    "                Theta_est = out.get(\"prec\", None)\n",
    "                if W_est is None:\n",
    "                    W_est = np.zeros((X.shape[1], X.shape[1]))\n",
    "                return W_est, Theta_est\n",
    "            else:\n",
    "                # generic function baseline like notears_linear(X, **args)\n",
    "                W_est = base.model(X, **base.args)\n",
    "                return W_est, None\n",
    "\n",
    "        # class-style baseline\n",
    "        model = base.model(**base.init) if base.init else base.model()\n",
    "        # Try to pass standardized X only if the baseline asked for it (already handled by caller via X argument)\n",
    "        model.fit(X, **base.args)\n",
    "\n",
    "        # Extract adjacency\n",
    "        W_est = getattr(model, \"W_est\", None)\n",
    "        if W_est is None:\n",
    "            # Some models might return directly from fit (rare); fallback to zeros\n",
    "            W_est = np.zeros((X.shape[1], X.shape[1]))\n",
    "\n",
    "        # Extract precision if present\n",
    "        Theta_est = None\n",
    "        if hasattr(model, \"Theta_est\"):\n",
    "            Theta_est = getattr(model, \"Theta_est\")\n",
    "        elif hasattr(model, \"prec\"):\n",
    "            Theta_est = getattr(model, \"prec\")\n",
    "\n",
    "        # TopoGreedy (non-refresh) variants sometimes return A transposed relative to convention;\n",
    "        # caller can request topo_transpose=True in BaselineSpec to fix orientation.\n",
    "        return W_est, Theta_est\n",
    "\n",
    "    def _save_manifest(self):\n",
    "        \"\"\"Save config manifest (JSON) for reproducibility, without non-serializable fields.\"\"\"\n",
    "        # build a sanitized copy of the config:\n",
    "        #   - convert baselines to lightweight dicts (stringify the model)\n",
    "        sanitized_cfg = {\n",
    "            \"n_graphs\": self.cfg.n_graphs,\n",
    "            \"n_nodes\": self.cfg.n_nodes,\n",
    "            \"n_samples_list\": list(self.cfg.n_samples_list),\n",
    "            \"edge_threshold\": self.cfg.edge_threshold,\n",
    "            \"data_params\": _to_jsonable(self.cfg.data_params),\n",
    "            \"baselines\": [\n",
    "                {\n",
    "                    \"name\": b.name,\n",
    "                    \"model\": _to_jsonable(b.model),           # \"module.ClassOrFunc\"\n",
    "                    \"init\": _to_jsonable(b.init),\n",
    "                    \"args\": _to_jsonable(b.args),\n",
    "                    \"standardize\": b.standardize,\n",
    "                    \"adapt_lambda\": b.adapt_lambda,\n",
    "                    \"topo_transpose\": b.topo_transpose,\n",
    "                    \"is_topogreedy_refresh\": b.is_topogreedy_refresh,\n",
    "                }\n",
    "                for b in self.cfg.baselines\n",
    "            ],\n",
    "            \"out_dir\": str(self.cfg.out_dir),\n",
    "            \"run_tag\": self.cfg.run_tag,\n",
    "            \"save_intermediate\": self.cfg.save_intermediate,\n",
    "            \"seed_offset\": self.cfg.seed_offset,\n",
    "        }\n",
    "\n",
    "        manifest = {\n",
    "            \"run_id\": self.run_id,\n",
    "            \"created_at\": dt.datetime.now().isoformat(),\n",
    "            \"config\": sanitized_cfg,\n",
    "            \"python\": {\"numpy_version\": np.__version__},\n",
    "        }\n",
    "\n",
    "        (self.out_root / \"config.json\").write_text(json.dumps(_to_jsonable(manifest), indent=2))\n",
    "\n",
    "    def _save_block(\n",
    "        self, g: int, si: int,\n",
    "        W_true: np.ndarray, Theta_true: np.ndarray,\n",
    "        W_est_all: np.ndarray,\n",
    "        Theta_est_all: np.ndarray,\n",
    "        shd: np.ndarray, tpr: np.ndarray, fdr: np.ndarray,\n",
    "        f1: np.ndarray, err: np.ndarray, rt: np.ndarray,\n",
    "        dags: np.ndarray, theta_diff: np.ndarray\n",
    "    ):\n",
    "        \"\"\"Persist per-(graph, sample-index) blob for debugging/inspection.\"\"\"\n",
    "        sub = self.out_root / f\"graph_{g:03d}\" / f\"samples_{self.cfg.n_samples_list[si]}\"\n",
    "        sub.mkdir(parents=True, exist_ok=True)\n",
    "        filename = sub / \"block.npz\"\n",
    "        np.savez_compressed(\n",
    "            filename,\n",
    "            W_true=W_true, Theta_true=Theta_true,\n",
    "            W_est_all=W_est_all, Theta_est_all=Theta_est_all,\n",
    "            shd=shd, tpr=tpr, fdr=fdr, f1=f1, err=err, runtime=rt,\n",
    "            dag_count=dags, theta_diff=theta_diff\n",
    "        )\n",
    "        \n",
    "        # Return the complete data as a dictionary containing all arrays\n",
    "        return {\n",
    "            'filename': str(filename),\n",
    "            'W_true': W_true,\n",
    "            'Theta_true': Theta_true,\n",
    "            'W_est_all': W_est_all,\n",
    "            'Theta_est_all': Theta_est_all,\n",
    "            'shd': shd,\n",
    "            'tpr': tpr,\n",
    "            'fdr': fdr,\n",
    "            'f1': f1,\n",
    "            'err': err,\n",
    "            'runtime': rt,\n",
    "            'dag_count': dags,\n",
    "            'theta_diff': theta_diff\n",
    "        }\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
